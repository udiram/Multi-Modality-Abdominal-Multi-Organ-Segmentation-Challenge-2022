{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "\n",
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import tempfile\n",
    "from glob import glob\n",
    "\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import monai\n",
    "from monai.data import create_test_image_3d, list_data_collate, decollate_batch\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsChannelFirstd,\n",
    "    AsDiscrete,\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    RandCropByPosNegLabeld,\n",
    "    RandRotate90d,\n",
    "    ScaleIntensityd,\n",
    "    EnsureTyped,\n",
    "    EnsureType,\n",
    ")\n",
    "from monai.visualize import plot_2d_or_3d_image"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 0.9.0\n",
      "Numpy version: 1.23.1\n",
      "Pytorch version: 1.12.0+cpu\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False\n",
      "MONAI rev id: af0e0e9f757558d144b655c63afcea3a4e0a06f5\n",
      "MONAI __file__: C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 4.0.1\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 9.2.0\n",
      "Tensorboard version: 2.9.1\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "tqdm version: 4.64.0\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.1\n",
      "pandas version: 1.4.3\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "monai.config.print_config()\n",
    "logging.basicConfig(stream=sys.stdout, level=logging.INFO)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "images = sorted(glob(os.path.join('data/CT_files/imagesTr/', \"img*.nii.gz\")))\n",
    "segs = sorted(glob(os.path.join('data/CT_files/labelsTr/', \"seg*.nii.gz\")))\n",
    "train_files = [{\"img\": img, \"seg\": seg} for img, seg in zip(images[:160], segs[:160])]\n",
    "val_files = [{\"img\": img, \"seg\": seg} for img, seg in zip(images[-40:], segs[-40:])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(len(train_files))\n",
    "print(len(val_files))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# define transforms for image and segmentation\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\", \"seg\"]),\n",
    "        AsChannelFirstd(keys=[\"img\", \"seg\"], channel_dim=-1),\n",
    "        ScaleIntensityd(keys=\"img\"),\n",
    "        RandCropByPosNegLabeld(\n",
    "            keys=[\"img\", \"seg\"], label_key=\"seg\", spatial_size=[96, 96, 96], pos=1, neg=1, num_samples=4\n",
    "        ),\n",
    "        RandRotate90d(keys=[\"img\", \"seg\"], prob=0.5, spatial_axes=[0, 2]),\n",
    "        EnsureTyped(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"img\", \"seg\"]),\n",
    "        AsChannelFirstd(keys=[\"img\", \"seg\"], channel_dim=-1),\n",
    "        ScaleIntensityd(keys=\"img\"),\n",
    "        EnsureTyped(keys=[\"img\", \"seg\"]),\n",
    "    ]\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\transform.py\", line 89, in apply_transform\n    return _apply_transform(transform, data, unpack_items)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\transform.py\", line 53, in _apply_transform\n    return transform(parameters)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\croppad\\dictionary.py\", line 1171, in __call__\n    self.randomize(label, fg_indices, bg_indices, image)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\croppad\\dictionary.py\", line 1153, in randomize\n    self.centers = generate_pos_neg_label_crop_centers(\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\utils.py\", line 512, in generate_pos_neg_label_crop_centers\n    centers.append(correct_crop_centers(center, spatial_size, label_spatial_shape, allow_smaller))\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\utils.py\", line 436, in correct_crop_centers\n    spatial_size = fall_back_tuple(spatial_size, default=label_spatial_shape)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\utils\\misc.py\", line 197, in fall_back_tuple\n    user = ensure_tuple_rep(user_provided, ndim)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\utils\\misc.py\", line 154, in ensure_tuple_rep\n    raise ValueError(f\"Sequence must have length {dim}, got {len(tup)}.\")\nValueError: Sequence must have length 2, got 3.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\transform.py\", line 89, in apply_transform\n    return _apply_transform(transform, data, unpack_items)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\transform.py\", line 53, in _apply_transform\n    return transform(parameters)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\compose.py\", line 173, in __call__\n    input_ = apply_transform(_transform, input_, self.map_items, self.unpack_items, self.log_stats)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\transform.py\", line 113, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.croppad.dictionary.RandCropByPosNegLabeld object at 0x00000274FF5C7610>\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\data\\dataset.py\", line 97, in __getitem__\n    return self._transform(index)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\data\\dataset.py\", line 83, in _transform\n    return apply_transform(self.transform, data_i) if self.transform is not None else data_i\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\transform.py\", line 113, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.compose.Compose object at 0x00000274C54BB520>\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [20]\u001B[0m, in \u001B[0;36m<cell line: 5>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;66;03m# use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\u001B[39;00m\n\u001B[0;32m      4\u001B[0m check_loader \u001B[38;5;241m=\u001B[39m DataLoader(check_ds, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m, collate_fn\u001B[38;5;241m=\u001B[39mlist_data_collate)\n\u001B[1;32m----> 5\u001B[0m check_data \u001B[38;5;241m=\u001B[39m \u001B[43mmonai\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmisc\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfirst\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcheck_loader\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28mprint\u001B[39m(check_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimg\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mshape, check_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mseg\u001B[39m\u001B[38;5;124m\"\u001B[39m]\u001B[38;5;241m.\u001B[39mshape)\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\utils\\misc.py:82\u001B[0m, in \u001B[0;36mfirst\u001B[1;34m(iterable, default)\u001B[0m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfirst\u001B[39m(iterable, default\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m     79\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m     80\u001B[0m \u001B[38;5;124;03m    Returns the first item in the given iterable or `default` if empty, meaningful mostly with 'for' expressions.\u001B[39;00m\n\u001B[0;32m     81\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m---> 82\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[0;32m     83\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m i\n\u001B[0;32m     84\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m default\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:652\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    649\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    650\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    651\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 652\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    653\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    654\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    655\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    656\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1347\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m   1345\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1346\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_task_info[idx]\n\u001B[1;32m-> 1347\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_process_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1373\u001B[0m, in \u001B[0;36m_MultiProcessingDataLoaderIter._process_data\u001B[1;34m(self, data)\u001B[0m\n\u001B[0;32m   1371\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_try_put_index()\n\u001B[0;32m   1372\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, ExceptionWrapper):\n\u001B[1;32m-> 1373\u001B[0m     \u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreraise\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1374\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m data\n",
      "File \u001B[1;32m~\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\torch\\_utils.py:461\u001B[0m, in \u001B[0;36mExceptionWrapper.reraise\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    457\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m:\n\u001B[0;32m    458\u001B[0m     \u001B[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001B[39;00m\n\u001B[0;32m    459\u001B[0m     \u001B[38;5;66;03m# instantiate since we don't know how to\u001B[39;00m\n\u001B[0;32m    460\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(msg) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m--> 461\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m exception\n",
      "\u001B[1;31mRuntimeError\u001B[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\transform.py\", line 89, in apply_transform\n    return _apply_transform(transform, data, unpack_items)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\transform.py\", line 53, in _apply_transform\n    return transform(parameters)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\croppad\\dictionary.py\", line 1171, in __call__\n    self.randomize(label, fg_indices, bg_indices, image)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\croppad\\dictionary.py\", line 1153, in randomize\n    self.centers = generate_pos_neg_label_crop_centers(\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\utils.py\", line 512, in generate_pos_neg_label_crop_centers\n    centers.append(correct_crop_centers(center, spatial_size, label_spatial_shape, allow_smaller))\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\utils.py\", line 436, in correct_crop_centers\n    spatial_size = fall_back_tuple(spatial_size, default=label_spatial_shape)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\utils\\misc.py\", line 197, in fall_back_tuple\n    user = ensure_tuple_rep(user_provided, ndim)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\utils\\misc.py\", line 154, in ensure_tuple_rep\n    raise ValueError(f\"Sequence must have length {dim}, got {len(tup)}.\")\nValueError: Sequence must have length 2, got 3.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\transform.py\", line 89, in apply_transform\n    return _apply_transform(transform, data, unpack_items)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\transform.py\", line 53, in _apply_transform\n    return transform(parameters)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\compose.py\", line 173, in __call__\n    input_ = apply_transform(_transform, input_, self.map_items, self.unpack_items, self.log_stats)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\transform.py\", line 113, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.croppad.dictionary.RandCropByPosNegLabeld object at 0x00000274FF5C7610>\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\worker.py\", line 302, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py\", line 49, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\data\\dataset.py\", line 97, in __getitem__\n    return self._transform(index)\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\data\\dataset.py\", line 83, in _transform\n    return apply_transform(self.transform, data_i) if self.transform is not None else data_i\n  File \"C:\\Users\\sathy\\OneDrive\\Documents\\GitHub\\Multi-Modality-Abdominal-Multi-Organ-Segmentation-Challenge-2022\\venv\\lib\\site-packages\\monai\\transforms\\transform.py\", line 113, in apply_transform\n    raise RuntimeError(f\"applying transform {transform}\") from e\nRuntimeError: applying transform <monai.transforms.compose.Compose object at 0x00000274C54BB520>\n"
     ]
    }
   ],
   "source": [
    "# define dataset, data loader\n",
    "check_ds = monai.data.Dataset(data=train_files, transform=train_transforms)\n",
    "# use batch_size=2 to load images and use RandCropByPosNegLabeld to generate 2 x 4 images for network training\n",
    "check_loader = DataLoader(check_ds, batch_size=2, num_workers=4, collate_fn=list_data_collate)\n",
    "check_data = monai.utils.misc.first(check_loader)\n",
    "print(check_data[\"img\"].shape, check_data[\"seg\"].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}